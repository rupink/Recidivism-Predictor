{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNaU3NRcahVF"
   },
   "source": [
    "## Part 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mbaC6r0maQaP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch \n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.modules.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PkfSf2gLbDq4"
   },
   "outputs": [],
   "source": [
    "link = 'https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv'\n",
    "readlink = pd.read_csv(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHPEqEFiskgf",
    "outputId": "f63de95d-7cfc-435f-dc45-05017efc0a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#GPU Computation\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U1EqCrfr8uK"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqEmKm7RcdGt"
   },
   "source": [
    "\"Participants saw a short description of a defendant that included the defendantâ€™s sex, age, and previous criminal history, but not their race\"\n",
    "\n",
    "I will use the sex and age of the defendant to train the regression model. For sex, I will turn all data into integer values first to make them easily iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0N7nHomioT1"
   },
   "outputs": [],
   "source": [
    "#get sex from data\n",
    "x_axis = readlink[[\"sex\"]].copy()\n",
    "\n",
    "#convert sex to int type\n",
    "arr = []\n",
    "for i in x_axis[\"sex\"]:\n",
    "  if i == \"Female\":\n",
    "    arr.append(0)\n",
    "  else:\n",
    "    arr.append(1)\n",
    "\n",
    "#append to all data\n",
    "x_axis[\"sex\"] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFrr1qejciEV"
   },
   "outputs": [],
   "source": [
    "#add all remaining integer values\n",
    "x_axis[\"age\"] = readlink[[\"age\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BLfGwzrjy65"
   },
   "outputs": [],
   "source": [
    "#set y axis as the two year recidivism\n",
    "y_axis = readlink[[\"two_year_recid\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bk2br8UshBFA"
   },
   "outputs": [],
   "source": [
    "#move data to the device \n",
    "new_x_axis = torch.tensor(x_axis.values).to(device)\n",
    "new_y_axis = torch.tensor(y_axis.values).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQXRFO-Ulxsx"
   },
   "source": [
    "\"The results in columns (A), (B), and (C) correspond to the average testing accuracy over 1000 random 80%/20% training/testing splits.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQ38-4UVlq3o"
   },
   "outputs": [],
   "source": [
    "#find the index at which to split the data\n",
    "val = np.floor(len(new_x_axis)*0.8)\n",
    "\n",
    "#first 80% of the data\n",
    "x_train = new_x_axis[:val.astype(int)].to(device)\n",
    "y_train = new_y_axis[:val.astype(int)].to(device)\n",
    "\n",
    "#last 20% of the data\n",
    "x_test = new_x_axis[val.astype(int):].to(device)\n",
    "y_test = new_y_axis[val.astype(int):].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sH1tGjR3nq2f"
   },
   "outputs": [],
   "source": [
    "#training and testing data initialization: used from pytorch github tutorial\n",
    "class TrainandTestDataset(data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(TrainandTestDataset, self).__init__()\n",
    "        assert x.shape[0] == y.shape[0]\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ayQcwxBco1v"
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(TrainandTestDataset(x_train, y_train), batch_size=32, shuffle = True)\n",
    "testloader = torch.utils.data.DataLoader(TrainandTestDataset(x_test, y_test), batch_size=32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMJQeg_HpAwf"
   },
   "outputs": [],
   "source": [
    "#Logistic Regression Class\n",
    "class LogisticRegression(nn.Module):\n",
    "  def __init__(self, n_input, n_output):\n",
    "    super(LogisticRegression, self).__init__()\n",
    "    self.linear = nn.Linear(n_input, n_output)\n",
    "  def forward(self,x):\n",
    "    result = torch.sigmoid(self.linear(x))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTl0eKDskr5N"
   },
   "outputs": [],
   "source": [
    "# declare the model\n",
    "model = LogisticRegression(2, 1).to(device = 'cuda')\n",
    "\n",
    "# define the criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# select the optimizer and pass to it the parameters of the model it will optimize\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLGXfsG8u5-o",
    "outputId": "df18bb35-d574-4f74-9b38-f44a628f45c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss= 0.21140483021736145\n",
      "epoch: 20 loss= 0.19770236313343048\n",
      "epoch: 40 loss= 0.2765422761440277\n",
      "epoch: 60 loss= 0.2996476888656616\n",
      "epoch: 80 loss= 0.25924918055534363\n",
      "epoch: 100 loss= 0.24272462725639343\n",
      "epoch: 120 loss= 0.25439873337745667\n",
      "epoch: 140 loss= 0.29320311546325684\n",
      "epoch: 160 loss= 0.22930343449115753\n",
      "epoch: 180 loss= 0.24006009101867676\n",
      "epoch: 200 loss= 0.22071579098701477\n",
      "epoch: 220 loss= 0.1891176998615265\n",
      "epoch: 240 loss= 0.20026583969593048\n",
      "epoch: 260 loss= 0.32082152366638184\n",
      "epoch: 280 loss= 0.20555579662322998\n",
      "epoch: 300 loss= 0.30765458941459656\n",
      "epoch: 320 loss= 0.2087249457836151\n",
      "epoch: 340 loss= 0.2704949975013733\n",
      "epoch: 360 loss= 0.2545343339443207\n",
      "epoch: 380 loss= 0.20040848851203918\n",
      "epoch: 400 loss= 0.19837163388729095\n",
      "epoch: 420 loss= 0.26728036999702454\n",
      "epoch: 440 loss= 0.34621644020080566\n",
      "epoch: 460 loss= 0.27221614122390747\n",
      "epoch: 480 loss= 0.22132067382335663\n",
      "epoch: 500 loss= 0.3074251711368561\n",
      "epoch: 520 loss= 0.21839582920074463\n",
      "epoch: 540 loss= 0.23728492856025696\n",
      "epoch: 560 loss= 0.23569124937057495\n",
      "epoch: 580 loss= 0.23014459013938904\n",
      "epoch: 600 loss= 0.19372625648975372\n",
      "epoch: 620 loss= 0.31831416487693787\n",
      "epoch: 640 loss= 0.31804725527763367\n",
      "epoch: 660 loss= 0.1820751577615738\n",
      "epoch: 680 loss= 0.1567675918340683\n",
      "epoch: 700 loss= 0.2687130272388458\n",
      "epoch: 720 loss= 0.24119481444358826\n",
      "epoch: 740 loss= 0.21024227142333984\n",
      "epoch: 760 loss= 0.2613467276096344\n",
      "epoch: 780 loss= 0.2381628304719925\n",
      "epoch: 800 loss= 0.22634537518024445\n",
      "epoch: 820 loss= 0.19985215365886688\n",
      "epoch: 840 loss= 0.2499634176492691\n",
      "epoch: 860 loss= 0.26196175813674927\n",
      "epoch: 880 loss= 0.2449054718017578\n",
      "epoch: 900 loss= 0.21422235667705536\n",
      "epoch: 920 loss= 0.22242414951324463\n",
      "epoch: 940 loss= 0.17798340320587158\n",
      "epoch: 960 loss= 0.2108885645866394\n",
      "epoch: 980 loss= 0.2728072702884674\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    for i, (x_i, y_i) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()           # cleans the gradients   \n",
    "        y_hat_i = model(x_i.float())            # forward pass\n",
    "        loss = criterion(y_hat_i, y_i.float())  # compute the loss and perform the backward pass\n",
    "        loss.backward()                 # computes the gradients\n",
    "        optimizer.step()                # update the parameters\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "      print(\"epoch:\", epoch, \"loss=\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9qSoPB5d9LN",
    "outputId": "da3dcb87-a8f9-406b-90d3-25f2d48690ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.987534761428833\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    for k, (x_k, y_k) in enumerate(testloader):\n",
    "        y_hat_k = model(x_k.float())\n",
    "        loss_test = criterion(y_hat_k, y_k.float())\n",
    "        total_loss += float(loss_test)\n",
    "\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0Uv-Vfnri1a"
   },
   "source": [
    "# FPR Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLfQnH-0AetK"
   },
   "source": [
    "To calculate the FPR parity, we need to import race data from the provided url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aZwxX0D3DXi"
   },
   "outputs": [],
   "source": [
    "y2_pred = model(x_test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEcOBITfsM_q"
   },
   "outputs": [],
   "source": [
    "#get race from data\n",
    "race = readlink[[\"race\"]].copy()\n",
    "\n",
    "#convert race to int type\n",
    "arr = []\n",
    "for i in race[\"race\"]:\n",
    "  if i == \"African-American\":\n",
    "    arr.append(0)\n",
    "  elif i == \"Caucasian\":\n",
    "    arr.append(1)\n",
    "  else:\n",
    "    arr.append(-1)\n",
    "\n",
    "#append to all data\n",
    "race[\"race\"] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdhlY4v1spJR"
   },
   "outputs": [],
   "source": [
    "#move data to the device \n",
    "new_race = torch.tensor(race.values).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tm8RGP7m8kqT",
    "outputId": "a5b85236-0f78-4618-aef1-e0f85c8f3ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of defendants:  7214\n",
      "number of black defendants in the database:  3696\n",
      "number of white defendants in the database:  2454\n",
      "recidivated black defendants:  51.433982683982684\n",
      "recidivated white defendants:  39.36430317848411\n"
     ]
    }
   ],
   "source": [
    "#Find the total recidivated rates of Caucasian and African-Americans\n",
    "b = 0\n",
    "w = 0\n",
    "br = 0\n",
    "wr = 0\n",
    "\n",
    "for i in range(len(new_race)):\n",
    "  if new_race[i] == 0:\n",
    "    b += 1\n",
    "  if new_race[i] == 1:\n",
    "    w += 1\n",
    "  if new_race[i] == 0 and new_y_axis[i] == 1:\n",
    "    br += 1\n",
    "  if new_race[i] == 1 and new_y_axis[i] == 1:\n",
    "    wr += 1\n",
    "\n",
    "print(\"total number of defendants: \", len(new_race))\n",
    "print(\"number of black defendants in the database: \", b)\n",
    "print(\"number of white defendants in the database: \", w)\n",
    "print(\"recidivated black defendants: \", br/b*100)\n",
    "print(\"recidivated white defendants: \", wr/w*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKbuTc0S_JQ7"
   },
   "source": [
    "Note: As calculated above, the dataset can be concluded to be partially skewed, considering the significantly larger percentage of black defendants that are present in the data. As well, the percentage recidivated - grouped by race - is significantly different, with a percent difference of about 11%. These calculations can also be used to compare the models predictions to actual data present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUSalQqys0F9"
   },
   "outputs": [],
   "source": [
    "#split data based on paper: 80% training and 20% test\n",
    "\n",
    "#find the index at which to split the data\n",
    "val = np.floor(len(new_race)*0.8)\n",
    "\n",
    "#first 80% of the data\n",
    "race_train = new_race[:val.astype(int)].to(device)\n",
    "\n",
    "#last 20% of the data\n",
    "race_test = new_race[val.astype(int):].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8JT4i43hiaG"
   },
   "source": [
    "In this calculation, let 50% be used as the threshold (chosen arbitrarily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdwUaarxzBjH",
    "outputId": "04de54c6-3d46-4d9f-9c9b-b38ce526a075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of African-Americans that reoffend and were predicted to reoffend:  233\n",
      "# of African-Americans that did not reoffend and were predicted to not reoffend:  184\n",
      "# of African-Americans that did not reoffend but were predicted to reoffend:  165\n",
      "# of African-Americans that did reoffend but were predicted not to reoffend:  162\n",
      "PPV:  58.5427135678392 %\n",
      "NPV:  53.179190751445084 %\n",
      "False Positive Parity:  47.277936962750715 %\n"
     ]
    }
   ],
   "source": [
    "#Find # of African Americans that are predicted to reoffend vs. who actually reoffend\n",
    "actual = 0 #True Positive\n",
    "were_predicted = 0 #False Positive\n",
    "not_predicted = 0 #True Negative\n",
    "wrong_predict = 0 #False Negative\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "  if race_test[i] == 0: #If African-American\n",
    "    if y2_pred[i] >= 0.5: #If predicted to reoffend\n",
    "      if y_test[i] == 1: #did reoffend\n",
    "        actual +=1\n",
    "      if y_test[i] == 0: #did not reoffend\n",
    "        were_predicted += 1\n",
    "    if y2_pred[i] < 0.5: \n",
    "      if y_test[i] == 0: #did not reoffend \n",
    "        not_predicted += 1\n",
    "      if y_test[i] == 1: #did reoffend\n",
    "        wrong_predict += 1\n",
    "\n",
    "print(\"# of African-Americans that reoffend and were predicted to reoffend: \", actual)\n",
    "print(\"# of African-Americans that did not reoffend and were predicted to not reoffend: \", not_predicted)\n",
    "print(\"# of African-Americans that did not reoffend but were predicted to reoffend: \", were_predicted)\n",
    "print(\"# of African-Americans that did reoffend but were predicted not to reoffend: \", wrong_predict)\n",
    "print(\"PPV: \", actual/(actual + were_predicted)*100, \"%\")\n",
    "print(\"NPV: \", not_predicted/(not_predicted + wrong_predict)*100, \"%\")\n",
    "print(\"False Positive Parity: \", were_predicted/(were_predicted + not_predicted)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wEFi4yoz9UW",
    "outputId": "4b2b8b11-772c-40b8-8bce-7790c152d901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Caucasians that reoffend and were predicted to reoffend:  86\n",
      "# of Caucasians that did not reoffend and were predicted to not reoffend:  212\n",
      "# of Caucasians that did not reoffend but were predicted to reoffend:  86\n",
      "# of Caucasians that did reoffend but were predicted not to reoffend:  92\n",
      "PPV:  50.0 %\n",
      "NPV:  69.73684210526315 %\n",
      "False Positive Parity:  28.859060402684566 %\n"
     ]
    }
   ],
   "source": [
    "#Find # of Caucasian Americans that are predicted to reoffend vs. who actually reoffend\n",
    "actual = 0 #True Positive\n",
    "were_predicted = 0 #False Positive\n",
    "not_predicted = 0 #True Negative\n",
    "wrong_predict = 0 #False Negative\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "  if race_test[i] == 1: #If Caucasian\n",
    "    if y2_pred[i] >= 0.5: #If predicted to reoffend\n",
    "      if y_test[i] == 1: #did reoffend\n",
    "        actual +=1\n",
    "      if y_test[i] == 0: #did not reoffend\n",
    "        were_predicted += 1\n",
    "    if y2_pred[i] < 0.5: \n",
    "      if y_test[i] == 0: #did not reoffend \n",
    "        not_predicted += 1\n",
    "      if y_test[i] == 1: #did reoffend\n",
    "        wrong_predict += 1\n",
    "\n",
    "print(\"# of Caucasians that reoffend and were predicted to reoffend: \", actual)\n",
    "print(\"# of Caucasians that did not reoffend and were predicted to not reoffend: \", not_predicted)\n",
    "print(\"# of Caucasians that did not reoffend but were predicted to reoffend: \", were_predicted)\n",
    "print(\"# of Caucasians that did reoffend but were predicted not to reoffend: \", wrong_predict)\n",
    "print(\"PPV: \", actual/(actual + were_predicted)*100, \"%\")\n",
    "print(\"NPV: \", not_predicted/(not_predicted + wrong_predict)*100, \"%\")\n",
    "print(\"False Positive Parity: \", were_predicted/(were_predicted + not_predicted)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T7P958-aCLi"
   },
   "source": [
    "The claim that was discussed in the article stated, \"the likelihood of recidivism for any given score is the same regardless of race (calibration).\"\n",
    "\n",
    "As seen above in the calculations carried out to determine the calibration and recidivism of both black and white defendants, it is clear that FPR parity has a difference that does not satisfy the bounds.\n",
    "\n",
    "Following the paper of Corbett-Davies and Goel, adjusting the threshold can lead to satisfied FPR parity (but dissatisfied calibration). Next, we will attempt to adjust the threshold value to imitate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_gresBCjI3D",
    "outputId": "a39e9a09-db86-4981-e2a4-46eaeef09c21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of African-Americans that reoffend:  60\n",
      "# of African-Americans that did not reoffend:  313\n",
      "# of African-Americans that did not reoffend but were predicted to reoffend:  36\n",
      "# of African-Americans that did reoffend but were predicted not to reoffend:  335\n",
      "PPV:  62.5 %\n",
      "NPV:  48.30246913580247 %\n",
      "False Positive Parity:  10.315186246418339 %\n"
     ]
    }
   ],
   "source": [
    "#adjusting thresholds: use 60%\n",
    "\n",
    "#Find # of African Americans that are predicted to reoffend vs. who actually reoffend\n",
    "actual = 0 #True Positive\n",
    "were_predicted = 0 #False Positive\n",
    "not_predicted = 0 #True Negative\n",
    "wrong_predict = 0 #False Negative\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "  if race_test[i] == 0: #If African-American\n",
    "    if y2_pred[i] >= 0.6: #If predicted to reoffend\n",
    "      if y_test[i] == 1: #did reoffend\n",
    "        actual +=1\n",
    "      elif y_test[i] == 0: #did not reoffend\n",
    "        were_predicted += 1\n",
    "    elif y2_pred[i] < 0.6: \n",
    "      if y_test[i] == 0: #did not reoffend \n",
    "        not_predicted += 1\n",
    "      if y_test[i] == 1: #did reoffend\n",
    "        wrong_predict += 1\n",
    "\n",
    "print(\"# of African-Americans that reoffend: \", actual)\n",
    "print(\"# of African-Americans that did not reoffend: \", not_predicted)\n",
    "print(\"# of African-Americans that did not reoffend but were predicted to reoffend: \", were_predicted)\n",
    "print(\"# of African-Americans that did reoffend but were predicted not to reoffend: \", wrong_predict)\n",
    "print(\"PPV: \", actual/(actual + were_predicted)*100, \"%\")\n",
    "print(\"NPV: \", not_predicted/(not_predicted + wrong_predict)*100, \"%\")\n",
    "print(\"False Positive Parity: \", were_predicted/(were_predicted + not_predicted)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJoSGgicjab8",
    "outputId": "bb60321a-8d8d-4a90-b2cf-3fdca4c8b536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Caucasians that reoffend:  20\n",
      "# of Caucasians that did not reoffend:  282\n",
      "# of Caucasians that did not reoffend but were predicted to reoffend:  16\n",
      "# of Caucasians that did reoffend but were predicted not to reoffend:  158\n",
      "PPV:  55.55555555555556 %\n",
      "NPV:  64.0909090909091 %\n",
      "False Positive Parity:  5.369127516778524 %\n"
     ]
    }
   ],
   "source": [
    "#adjusting thresholds: use 60%\n",
    "\n",
    "#Find # of Caucasian Americans that are predicted to reoffend vs. who actually reoffend\n",
    "actual = 0 #True Positive\n",
    "were_predicted = 0 #False Positive\n",
    "not_predicted = 0 #True Negative\n",
    "wrong_predict = 0 #False Negative\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "  if race_test[i] == 1: #If Caucasian\n",
    "    if y2_pred[i] >= 0.6: #If predicted to reoffend\n",
    "      if y_test[i] == 1: #did reoffend\n",
    "        actual +=1\n",
    "      elif y_test[i] == 0: #did not reoffend\n",
    "        were_predicted += 1\n",
    "    elif y2_pred[i] < 0.6: \n",
    "      if y_test[i] == 0: #did not reoffend \n",
    "        not_predicted += 1\n",
    "      if y_test[i] == 1: #did reoffend\n",
    "        wrong_predict += 1\n",
    "\n",
    "print(\"# of Caucasians that reoffend: \", actual)\n",
    "print(\"# of Caucasians that did not reoffend: \", not_predicted)\n",
    "print(\"# of Caucasians that did not reoffend but were predicted to reoffend: \", were_predicted)\n",
    "print(\"# of Caucasians that did reoffend but were predicted not to reoffend: \", wrong_predict)\n",
    "print(\"PPV: \", actual/(actual + were_predicted)*100, \"%\")\n",
    "print(\"NPV: \", not_predicted/(not_predicted + wrong_predict)*100, \"%\")\n",
    "print(\"False Positive Parity: \", were_predicted/(were_predicted + not_predicted)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nJk3qt-_ztK"
   },
   "source": [
    "As seen above, adjusting the threshold merely 10% leads to a larger calibration difference and better FPR parity difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3Kf38ZReIm4"
   },
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjBiWMiLkhif"
   },
   "source": [
    "Since there are over 400 charge descriptions, use one hot encoding to classify them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qp1_xkWBmAWT"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Gx6kOmtiou_"
   },
   "outputs": [],
   "source": [
    "#Use one hot encodings for charge descriptions\n",
    "arr = []\n",
    "charge_description = readlink[[\"c_charge_desc\"]].copy()\n",
    "\n",
    "for i in charge_description[\"c_charge_desc\"]:\n",
    "  arr.append(i)\n",
    "\n",
    "arr = np.array(arr)\n",
    "\n",
    "#Label Encoder will turn these values into integer values\n",
    "int_vals = LabelEncoder().fit_transform(arr)\n",
    "\n",
    "#One Hot Encoder will create the respective one hot vectors\n",
    "one_hot_vector = OneHotEncoder(sparse = False).fit_transform(int_vals.reshape(7214, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUzFLXQasPJa"
   },
   "outputs": [],
   "source": [
    "x2_axis = readlink[[\"sex\"]].copy()\n",
    "\n",
    "#convert sex to int type\n",
    "arr = []\n",
    "for i in x2_axis[\"sex\"]:\n",
    "  if i == \"Female\":\n",
    "    arr.append(0)\n",
    "  else:\n",
    "    arr.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPca8iMVsZFV"
   },
   "outputs": [],
   "source": [
    "x2_axis = readlink[[\"c_charge_degree\"]].copy()\n",
    "\n",
    "#convert degree to int type\n",
    "arr2 = []\n",
    "for i in x2_axis[\"c_charge_degree\"]:\n",
    "  if i == \"F\":\n",
    "    arr2.append(0)\n",
    "  else:\n",
    "    arr2.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZFRn9wi2SvQ"
   },
   "outputs": [],
   "source": [
    "#append to all data\n",
    "x2_axis[\"sex\"] = arr\n",
    "x2_axis[\"c_charge_degree\"] = arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38tuBDrbfcvJ"
   },
   "outputs": [],
   "source": [
    "#get all other integer valued data\n",
    "x2_axis[[\"age\", \"juv_fel_count\",\"juv_misd_count\", \"priors_count\"]] = readlink[[\"age\", \"juv_fel_count\",\"juv_misd_count\", \"priors_count\"]].copy()\n",
    "y2_axis = readlink[[\"two_year_recid\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMAdg0xAr5PR"
   },
   "outputs": [],
   "source": [
    "#get race from data\n",
    "race = readlink[[\"race\"]].copy()\n",
    "\n",
    "#convert race to int type\n",
    "arr3 = []\n",
    "for i in race[\"race\"]:\n",
    "  if i == \"African-American\":\n",
    "    arr3.append(0)\n",
    "  elif i == \"Caucasian\":\n",
    "    arr3.append(1)\n",
    "  else:\n",
    "    arr3.append(-1)\n",
    "\n",
    "#append to all data\n",
    "race[\"race\"] = arr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDicScUpqhQI"
   },
   "outputs": [],
   "source": [
    "x2_axis = x2_axis.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOltJvRykMRp"
   },
   "outputs": [],
   "source": [
    "empty_arr = np.empty((7214, 444))\n",
    "\n",
    "for i in range(len(x2_axis)):\n",
    "  empty_arr[i] = np.concatenate((x2_axis[i], one_hot_vector[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRSUPCUjupAu"
   },
   "outputs": [],
   "source": [
    "#move data to the device \n",
    "new_x_axis = torch.from_numpy(empty_arr).to(device)\n",
    "new_y_axis = torch.tensor(y2_axis.values).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mkF8OdAu8eF"
   },
   "outputs": [],
   "source": [
    "#split data based on paper: 80% training and 20% test\n",
    "\n",
    "#find the indices at which to split the data\n",
    "val = np.floor(len(new_x_axis)*0.8)\n",
    "\n",
    "#first 80% of the data\n",
    "x_train = new_x_axis[:val.astype(int)].to(device)\n",
    "y_train = new_y_axis[:val.astype(int)].to(device)\n",
    "\n",
    "#last 20% of the data\n",
    "x_test = new_x_axis[val.astype(int):].to(device)\n",
    "y_test = new_y_axis[val.astype(int):].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fahQyqrzfmvX"
   },
   "outputs": [],
   "source": [
    "race_data = torch.tensor(race.values).to(device)\n",
    "\n",
    "#find the indices at which to split the data\n",
    "val = np.floor(len(race_data)*0.8)\n",
    "\n",
    "racetrain_dataset = race_data[:val.astype(int)].to(device)\n",
    "racetest_dataset = race_data[val.astype(int):].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISotckfkvqAb"
   },
   "outputs": [],
   "source": [
    "class MainNetwork(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MainNetwork, self).__init__()\n",
    "    self.network = nn.Sequential(nn.Linear(444, 1), nn.Sigmoid(), nn.Linear(1, 1))\n",
    "  def forward(self, x):\n",
    "    result = self.network(x)\n",
    "    return result\n",
    "\n",
    "class Adversarial(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Adversarial, self).__init__()\n",
    "    self.network = nn.Sequential(nn.Linear(1, 1), nn.Sigmoid())\n",
    "  def forward(self, x):\n",
    "    result = self.network(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYFUJIsRsDUg"
   },
   "source": [
    "Note that the adversarial learning procedure would need to be iterative: the adversary is optimized, and then the whole network N is optimized, in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOZ09VHjgZ8q"
   },
   "outputs": [],
   "source": [
    "#adversary\n",
    "train_loader2 = torch.utils.data.DataLoader(TrainandTestDataset(x_train, racetrain_dataset), batch_size=32, shuffle = True)\n",
    "test_loader2 = torch.utils.data.DataLoader(TrainandTestDataset(x_test, racetest_dataset), batch_size=32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi3ABDkHwHiy"
   },
   "outputs": [],
   "source": [
    "# declare the model\n",
    "adversary_model = Adversarial().to(device)\n",
    "\n",
    "# define the criterion\n",
    "adversary_criterion = nn.MSELoss()\n",
    "\n",
    "# select the optimizer and pass to it the parameters of the model it will optimize\n",
    "adversary_optimizer = torch.optim.Adam(adversary_model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFTmX0TxvXUg"
   },
   "outputs": [],
   "source": [
    "#network\n",
    "train_loader = torch.utils.data.DataLoader(TrainandTestDataset(x_train, y_train), batch_size=32, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(TrainandTestDataset(x_test, y_test), batch_size=32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi7cWLq4wCy9"
   },
   "outputs": [],
   "source": [
    "# declare the model\n",
    "network_model = MainNetwork().to(device)\n",
    "\n",
    "# define the criterion\n",
    "network_criterion = nn.MSELoss()\n",
    "\n",
    "# select the optimizer and pass to it the parameters of the model it will optimize\n",
    "network_optimizer = torch.optim.Adam(list(network_model.parameters()) + list(adversary_model.parameters()), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_aPoAk5867_",
    "outputId": "37a7ca54-4da4-4eca-9f51-5882305de303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss= 0.5745607018470764\n",
      "epoch: 20 loss= 0.5249937772750854\n",
      "epoch: 40 loss= 0.49114662408828735\n",
      "epoch: 60 loss= 0.47149041295051575\n",
      "epoch: 80 loss= 0.46106547117233276\n",
      "epoch: 100 loss= 0.45568031072616577\n",
      "epoch: 120 loss= 0.452875018119812\n",
      "epoch: 140 loss= 0.4513852894306183\n",
      "epoch: 160 loss= 0.4505802392959595\n",
      "epoch: 180 loss= 0.45014020800590515\n",
      "epoch: 200 loss= 0.4498986303806305\n",
      "epoch: 220 loss= 0.44976624846458435\n",
      "epoch: 240 loss= 0.4496941566467285\n",
      "epoch: 260 loss= 0.44965535402297974\n",
      "epoch: 280 loss= 0.44963476061820984\n",
      "epoch: 300 loss= 0.44962403178215027\n",
      "epoch: 320 loss= 0.44961848855018616\n",
      "epoch: 340 loss= 0.4496156573295593\n",
      "epoch: 360 loss= 0.44961410760879517\n",
      "epoch: 380 loss= 0.4496132731437683\n",
      "epoch: 400 loss= 0.44961273670196533\n",
      "epoch: 420 loss= 0.4496123492717743\n",
      "epoch: 440 loss= 0.44961199164390564\n",
      "epoch: 460 loss= 0.44961175322532654\n",
      "epoch: 480 loss= 0.4496114253997803\n",
      "epoch: 500 loss= 0.4496111273765564\n",
      "epoch: 520 loss= 0.4496108293533325\n",
      "epoch: 540 loss= 0.44961047172546387\n",
      "epoch: 560 loss= 0.4496101438999176\n",
      "epoch: 580 loss= 0.44960981607437134\n",
      "epoch: 600 loss= 0.4496094584465027\n",
      "epoch: 620 loss= 0.4496091306209564\n",
      "epoch: 640 loss= 0.44960880279541016\n",
      "epoch: 660 loss= 0.4496084451675415\n",
      "epoch: 680 loss= 0.44960805773735046\n",
      "epoch: 700 loss= 0.4496077001094818\n",
      "epoch: 720 loss= 0.44960731267929077\n",
      "epoch: 740 loss= 0.44960692524909973\n",
      "epoch: 760 loss= 0.4496065080165863\n",
      "epoch: 780 loss= 0.44960612058639526\n",
      "epoch: 800 loss= 0.44960567355155945\n",
      "epoch: 820 loss= 0.4496052861213684\n",
      "epoch: 840 loss= 0.449604868888855\n",
      "epoch: 860 loss= 0.4496043920516968\n",
      "epoch: 880 loss= 0.44960397481918335\n",
      "epoch: 900 loss= 0.44960349798202515\n",
      "epoch: 920 loss= 0.44960305094718933\n",
      "epoch: 940 loss= 0.4496026039123535\n",
      "epoch: 960 loss= 0.4496020972728729\n",
      "epoch: 980 loss= 0.4496016502380371\n"
     ]
    }
   ],
   "source": [
    "#Adversary\n",
    "epochs = 1000\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "        adversary_optimizer.zero_grad()           # cleans the gradients    \n",
    "\n",
    "        a = torch.sigmoid(network_model(x_train.float()))            # forward pass\n",
    "        b = adversary_model(a)            # forward pass\n",
    "  \n",
    "        adversary_loss = adversary_criterion(b, racetrain_dataset.float())  # compute the loss and perform the backward pass\n",
    "        adversary_loss.backward()                 # computes the gradients\n",
    "  \n",
    "        adversary_optimizer.step()                # update the parameters\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "          print(\"epoch:\", epoch, \"loss=\", adversary_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Zb5hOVXsJTx",
    "outputId": "ec9e7e05-7781-4f6c-f198-773c28da6d95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \tNetwork loss= 0.27019622921943665\n",
      "epoch: 20 \tNetwork loss= 0.24481698870658875\n",
      "epoch: 40 \tNetwork loss= 0.23268704116344452\n",
      "epoch: 60 \tNetwork loss= 0.22402913868427277\n",
      "epoch: 80 \tNetwork loss= 0.21756288409233093\n",
      "epoch: 100 \tNetwork loss= 0.2123938351869583\n",
      "epoch: 120 \tNetwork loss= 0.2081780880689621\n",
      "epoch: 140 \tNetwork loss= 0.20477011799812317\n",
      "epoch: 160 \tNetwork loss= 0.2020874321460724\n",
      "epoch: 180 \tNetwork loss= 0.20000535249710083\n",
      "epoch: 200 \tNetwork loss= 0.19839654862880707\n",
      "epoch: 220 \tNetwork loss= 0.1971520334482193\n",
      "epoch: 240 \tNetwork loss= 0.19618217647075653\n",
      "epoch: 260 \tNetwork loss= 0.19542206823825836\n",
      "epoch: 280 \tNetwork loss= 0.19482585787773132\n",
      "epoch: 300 \tNetwork loss= 0.19435547292232513\n",
      "epoch: 320 \tNetwork loss= 0.19397689402103424\n",
      "epoch: 340 \tNetwork loss= 0.19366015493869781\n",
      "epoch: 360 \tNetwork loss= 0.19337698817253113\n",
      "epoch: 380 \tNetwork loss= 0.19311535358428955\n",
      "epoch: 400 \tNetwork loss= 0.19288693368434906\n",
      "epoch: 420 \tNetwork loss= 0.19269251823425293\n",
      "epoch: 440 \tNetwork loss= 0.19252516329288483\n",
      "epoch: 460 \tNetwork loss= 0.19238156080245972\n",
      "epoch: 480 \tNetwork loss= 0.1922631710767746\n",
      "epoch: 500 \tNetwork loss= 0.1921621412038803\n",
      "epoch: 520 \tNetwork loss= 0.1920701116323471\n",
      "epoch: 540 \tNetwork loss= 0.19198760390281677\n",
      "epoch: 560 \tNetwork loss= 0.1919192373752594\n",
      "epoch: 580 \tNetwork loss= 0.19186589121818542\n",
      "epoch: 600 \tNetwork loss= 0.19182464480400085\n",
      "epoch: 620 \tNetwork loss= 0.1917920708656311\n",
      "epoch: 640 \tNetwork loss= 0.1917656660079956\n",
      "epoch: 660 \tNetwork loss= 0.19174371659755707\n",
      "epoch: 680 \tNetwork loss= 0.19172509014606476\n",
      "epoch: 700 \tNetwork loss= 0.1917089819908142\n",
      "epoch: 720 \tNetwork loss= 0.19169476628303528\n",
      "epoch: 740 \tNetwork loss= 0.1916821300983429\n",
      "epoch: 760 \tNetwork loss= 0.19167067110538483\n",
      "epoch: 780 \tNetwork loss= 0.1916600465774536\n",
      "epoch: 800 \tNetwork loss= 0.19164933264255524\n",
      "epoch: 820 \tNetwork loss= 0.19163109362125397\n",
      "epoch: 840 \tNetwork loss= 0.1915966123342514\n",
      "epoch: 860 \tNetwork loss= 0.1915716528892517\n",
      "epoch: 880 \tNetwork loss= 0.19155888259410858\n",
      "epoch: 900 \tNetwork loss= 0.1915503591299057\n",
      "epoch: 920 \tNetwork loss= 0.1915433704853058\n",
      "epoch: 940 \tNetwork loss= 0.19153721630573273\n",
      "epoch: 960 \tNetwork loss= 0.19153159856796265\n",
      "epoch: 980 \tNetwork loss= 0.1915263682603836\n"
     ]
    }
   ],
   "source": [
    "#Main Network \n",
    "epochs = 1000\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "  network_optimizer.zero_grad()\n",
    "\n",
    "  a = torch.sigmoid(network_model(x_train.float())) \n",
    "  b = adversary_model(a)\n",
    "\n",
    "  network_loss = network_criterion(a, y_train.float())\n",
    "  network_loss.backward(retain_graph = True)\n",
    "  \n",
    "  network_optimizer.step()\n",
    "\n",
    "  if (epoch) % 20 == 0: \n",
    "    print(\"epoch:\", epoch, \"\\tNetwork loss=\", network_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ljnWCszAFk2",
    "outputId": "861ce40e-23f9-42a5-9657-625365d7ec2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of African-American that reoffend:  138\n",
      "# of African-American that did not reoffend:  198\n",
      "# of African-American that did not reoffend but were predicted to reoffend:  151\n",
      "# of African-American that did reoffend but were predicted not to reoffend:  257\n",
      "PPV:  47.75086505190311 %\n",
      "NPV:  43.51648351648352 %\n",
      "False Positive Parity:  43.26647564469914 %\n"
     ]
    }
   ],
   "source": [
    "#Find # of African-Americans that are predicted to reoffend vs. who actually reoffend\n",
    "actual = 0 #True Positive\n",
    "were_predicted = 0 #False Positive\n",
    "not_predicted = 0 #True Negative\n",
    "wrong_predict = 0 #False Negative\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "  if race_test[i] == 0: #If African-American\n",
    "    if a[i] >= 0.5: #If predicted to reoffend\n",
    "      if y_test[i] == 1: #did reoffend\n",
    "        actual +=1\n",
    "      elif y_test[i] == 0: #did not reoffend \n",
    "        were_predicted += 1\n",
    "    elif a[i] < 0.5: \n",
    "      if y_test[i] == 0: #did not reoffend \n",
    "        not_predicted += 1\n",
    "      if y_test[i] == 1: #did reoffend\n",
    "        wrong_predict += 1\n",
    "\n",
    "print(\"# of African-American that reoffend: \", actual)\n",
    "print(\"# of African-American that did not reoffend: \", not_predicted)\n",
    "print(\"# of African-American that did not reoffend but were predicted to reoffend: \", were_predicted)\n",
    "print(\"# of African-American that did reoffend but were predicted not to reoffend: \", wrong_predict)\n",
    "print(\"PPV: \", actual/(actual + were_predicted)*100, \"%\")\n",
    "print(\"NPV: \", not_predicted/(not_predicted + wrong_predict)*100, \"%\")\n",
    "print(\"False Positive Parity: \", were_predicted/(were_predicted + not_predicted)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ahfQU-KA8BB",
    "outputId": "5d348e53-f8d3-4930-fc98-7940755ade3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Caucasians that reoffend:  66\n",
      "# of Caucasians that did not reoffend:  181\n",
      "# of Caucasians that did not reoffend but were predicted to reoffend:  117\n",
      "# of Caucasians that did reoffend but were predicted not to reoffend:  112\n",
      "PPV:  36.0655737704918 %\n",
      "NPV:  61.774744027303754 %\n",
      "False Positive Parity:  39.261744966442954 %\n"
     ]
    }
   ],
   "source": [
    "#Find # of Caucasian Americans that are predicted to reoffend vs. who actually reoffend\n",
    "actual = 0 #True Positive\n",
    "were_predicted = 0 #False Positive\n",
    "not_predicted = 0 #True Negative\n",
    "wrong_predict = 0 #False Negative\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "  if race_test[i] == 1: #If Caucasian\n",
    "    if a[i] >= 0.5: #If predicted to reoffend\n",
    "      if y_test[i] == 1: #did reoffend\n",
    "        actual +=1\n",
    "      elif y_test[i] == 0: #did not reoffend\n",
    "        were_predicted += 1\n",
    "    elif a[i] < 0.5: \n",
    "      if y_test[i] == 0: #did not reoffend \n",
    "        not_predicted += 1\n",
    "      if y_test[i] == 1: #did reoffend\n",
    "        wrong_predict += 1\n",
    "\n",
    "print(\"# of Caucasians that reoffend: \", actual)\n",
    "print(\"# of Caucasians that did not reoffend: \", not_predicted)\n",
    "print(\"# of Caucasians that did not reoffend but were predicted to reoffend: \", were_predicted)\n",
    "print(\"# of Caucasians that did reoffend but were predicted not to reoffend: \", wrong_predict)\n",
    "print(\"PPV: \", actual/(actual + were_predicted)*100, \"%\")\n",
    "print(\"NPV: \", not_predicted/(not_predicted + wrong_predict)*100, \"%\")\n",
    "print(\"False Positive Parity: \", were_predicted/(were_predicted + not_predicted)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tH0mwlvAHpM"
   },
   "source": [
    "The following seven features were used: \n",
    "- age\n",
    "- juv_fel_count\n",
    "- juv_misd_count\n",
    "- priors_count\n",
    "- sex\n",
    "- c_charge_degree\n",
    "- c_charge_desc\n",
    "\n",
    "For the first five listed above, nothing was done. For sex and c_charge_desc, these string values were converted to integer values, which was easy due to their binary manner. The last one was difficult as there were over 400 unique charges. In order to combat this issue, one hot vectors were used where each unique charge was given its own unique vector that could be used in training and testing.\n",
    "\n",
    "The adversarial model followed the given procedure:\n",
    "- Train the adversaral learning model: Linear->Sigmoid\n",
    "- Train the network: Linear->Sigmoid->Linear->Adversal->Sigmoid\n",
    "\n",
    "A threshold of 50% was used for this trial, in order to compare the results to part 1, where only two attributes were used. As seen above, the FPR parity is < 5%, although the process in which the parity was calculated is the same as done in part 1. This shows that this model as well as increasing the number of features can improve these results by providing consistency between races in prediction calculations."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
