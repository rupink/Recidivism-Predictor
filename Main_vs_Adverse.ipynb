{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Train_Test_Loops import *\n",
    "from Imports import *\n",
    "from LogisticRegression import *\n",
    "from Data_Format import *\n",
    "from TrainandTestDataset import * \n",
    "from Main_Adver_Network import *\n",
    "from Train_Advs_Main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data_Format_2(CONTENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adversary\n",
    "train_loader2 = torch.utils.data.DataLoader(TrainandTestDataset(Data.x_train, Data.racetrain_dataset), batch_size=32, shuffle = True)\n",
    "test_loader2 = torch.utils.data.DataLoader(TrainandTestDataset(Data.x_test, Data.racetest_dataset), batch_size=32, shuffle = False)\n",
    "\n",
    "# declare the model\n",
    "adversary_model = Adversarial()\n",
    "\n",
    "# define the criterion\n",
    "adversary_criterion = nn.MSELoss()\n",
    "\n",
    "# select the optimizer and pass to it the parameters of the model it will optimize\n",
    "adversary_optimizer = torch.optim.Adam(adversary_model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network\n",
    "train_loader = torch.utils.data.DataLoader(TrainandTestDataset(Data.x_train,Data.y_train), batch_size=32, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(TrainandTestDataset(Data.x_test, Data.y_test), batch_size=32, shuffle = False)\n",
    "\n",
    "# declare the model\n",
    "network_model = MainNetwork()\n",
    "\n",
    "# define the criterion\n",
    "network_criterion = nn.MSELoss()\n",
    "\n",
    "# select the optimizer and pass to it the parameters of the model it will optimize\n",
    "network_optimizer = torch.optim.Adam(list(network_model.parameters()) + list(adversary_model.parameters()), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss= 0.6502230167388916\n",
      "epoch: 20 loss= 0.5912430882453918\n",
      "epoch: 40 loss= 0.5425915122032166\n",
      "epoch: 60 loss= 0.5074684023857117\n",
      "epoch: 80 loss= 0.48468419909477234\n",
      "epoch: 100 loss= 0.4708135724067688\n",
      "epoch: 120 loss= 0.4625716507434845\n",
      "epoch: 140 loss= 0.45766928791999817\n",
      "epoch: 160 loss= 0.4547155797481537\n",
      "epoch: 180 loss= 0.4529072940349579\n",
      "epoch: 200 loss= 0.45178377628326416\n",
      "epoch: 220 loss= 0.45107749104499817\n",
      "epoch: 240 loss= 0.4506298303604126\n",
      "epoch: 260 loss= 0.45034468173980713\n",
      "epoch: 280 loss= 0.4501626491546631\n",
      "epoch: 300 loss= 0.4500465989112854\n",
      "epoch: 320 loss= 0.44997286796569824\n",
      "epoch: 340 loss= 0.4499262869358063\n",
      "epoch: 360 loss= 0.44989699125289917\n",
      "epoch: 380 loss= 0.44987863302230835\n",
      "epoch: 400 loss= 0.4498673975467682\n",
      "epoch: 420 loss= 0.44986051321029663\n",
      "epoch: 440 loss= 0.44985631108283997\n",
      "epoch: 460 loss= 0.4498538374900818\n",
      "epoch: 480 loss= 0.44985225796699524\n",
      "epoch: 500 loss= 0.4498513340950012\n",
      "epoch: 520 loss= 0.44985076785087585\n",
      "epoch: 540 loss= 0.44985032081604004\n",
      "epoch: 560 loss= 0.44985002279281616\n",
      "epoch: 580 loss= 0.4498497247695923\n",
      "epoch: 600 loss= 0.44984951615333557\n",
      "epoch: 620 loss= 0.449849396944046\n",
      "epoch: 640 loss= 0.4498492479324341\n",
      "epoch: 660 loss= 0.44984903931617737\n",
      "epoch: 680 loss= 0.44984889030456543\n",
      "epoch: 700 loss= 0.4498487412929535\n",
      "epoch: 720 loss= 0.4498485326766968\n",
      "epoch: 740 loss= 0.4498482942581177\n",
      "epoch: 760 loss= 0.44984811544418335\n",
      "epoch: 780 loss= 0.4498479962348938\n",
      "epoch: 800 loss= 0.4498478174209595\n",
      "epoch: 820 loss= 0.44984763860702515\n",
      "epoch: 840 loss= 0.44984742999076843\n",
      "epoch: 860 loss= 0.4498472809791565\n",
      "epoch: 880 loss= 0.449847012758255\n",
      "epoch: 900 loss= 0.4498468339443207\n",
      "epoch: 920 loss= 0.4498465955257416\n",
      "epoch: 940 loss= 0.44984641671180725\n",
      "epoch: 960 loss= 0.44984620809555054\n",
      "epoch: 980 loss= 0.4498460590839386\n"
     ]
    }
   ],
   "source": [
    "Train_Adv(epochs, network_model, Data.x_train, Data.racetrain_dataset, adversary_model, adversary_optimizer, adversary_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \tNetwork loss= 0.251923143863678\n",
      "epoch: 20 \tNetwork loss= 0.231510192155838\n",
      "epoch: 40 \tNetwork loss= 0.223555326461792\n",
      "epoch: 60 \tNetwork loss= 0.21756064891815186\n",
      "epoch: 80 \tNetwork loss= 0.21265177428722382\n",
      "epoch: 100 \tNetwork loss= 0.20865337550640106\n",
      "epoch: 120 \tNetwork loss= 0.2054428905248642\n",
      "epoch: 140 \tNetwork loss= 0.20289890468120575\n",
      "epoch: 160 \tNetwork loss= 0.20088869333267212\n",
      "epoch: 180 \tNetwork loss= 0.19928763806819916\n",
      "epoch: 200 \tNetwork loss= 0.19799846410751343\n",
      "epoch: 220 \tNetwork loss= 0.19695325195789337\n",
      "epoch: 240 \tNetwork loss= 0.19610367715358734\n",
      "epoch: 260 \tNetwork loss= 0.19541534781455994\n",
      "epoch: 280 \tNetwork loss= 0.1948549896478653\n",
      "epoch: 300 \tNetwork loss= 0.19439104199409485\n",
      "epoch: 320 \tNetwork loss= 0.194001704454422\n",
      "epoch: 340 \tNetwork loss= 0.193677619099617\n",
      "epoch: 360 \tNetwork loss= 0.19341062009334564\n",
      "epoch: 380 \tNetwork loss= 0.19318421185016632\n",
      "epoch: 400 \tNetwork loss= 0.19297818839550018\n",
      "epoch: 420 \tNetwork loss= 0.1927795261144638\n",
      "epoch: 440 \tNetwork loss= 0.19258973002433777\n",
      "epoch: 460 \tNetwork loss= 0.19241894781589508\n",
      "epoch: 480 \tNetwork loss= 0.19226811826229095\n",
      "epoch: 500 \tNetwork loss= 0.19212913513183594\n",
      "epoch: 520 \tNetwork loss= 0.19200947880744934\n",
      "epoch: 540 \tNetwork loss= 0.19191443920135498\n",
      "epoch: 560 \tNetwork loss= 0.1918371170759201\n",
      "epoch: 580 \tNetwork loss= 0.19176964461803436\n",
      "epoch: 600 \tNetwork loss= 0.19170616567134857\n",
      "epoch: 620 \tNetwork loss= 0.1916516125202179\n",
      "epoch: 640 \tNetwork loss= 0.19161109626293182\n",
      "epoch: 660 \tNetwork loss= 0.19158092141151428\n",
      "epoch: 680 \tNetwork loss= 0.1915571540594101\n",
      "epoch: 700 \tNetwork loss= 0.19153755903244019\n",
      "epoch: 720 \tNetwork loss= 0.19152089953422546\n",
      "epoch: 740 \tNetwork loss= 0.191506490111351\n",
      "epoch: 760 \tNetwork loss= 0.19149373471736908\n",
      "epoch: 780 \tNetwork loss= 0.19148239493370056\n",
      "epoch: 800 \tNetwork loss= 0.1914721578359604\n",
      "epoch: 820 \tNetwork loss= 0.19146285951137543\n",
      "epoch: 840 \tNetwork loss= 0.19145429134368896\n",
      "epoch: 860 \tNetwork loss= 0.1914464235305786\n",
      "epoch: 880 \tNetwork loss= 0.1914391815662384\n",
      "epoch: 900 \tNetwork loss= 0.19143235683441162\n",
      "epoch: 920 \tNetwork loss= 0.19142597913742065\n",
      "epoch: 940 \tNetwork loss= 0.19142000377178192\n",
      "epoch: 960 \tNetwork loss= 0.19141441583633423\n",
      "epoch: 980 \tNetwork loss= 0.19140911102294922\n"
     ]
    }
   ],
   "source": [
    "(a) = Train_Main(epochs, network_model, Data.x_train,Data.y_train,network_optimizer, network_criterion, adversary_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get race from data\n",
    "race = CONTENTS[[\"race\"]].copy()\n",
    "\n",
    "#convert race to int type\n",
    "arr = []\n",
    "for i in race[\"race\"]:\n",
    "  if i == \"African-American\":\n",
    "    arr.append(0)\n",
    "  elif i == \"Caucasian\":\n",
    "    arr.append(1)\n",
    "  else:\n",
    "    arr.append(-1)\n",
    "\n",
    "#append to all data\n",
    "race[\"race\"] = arr\n",
    "new_race = torch.tensor(race.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data based on paper: 80% training and 20% test\n",
    "\n",
    "#find the index at which to split the data\n",
    "val = np.floor(len(new_race)*0.8)\n",
    "\n",
    "#first 80% of the data\n",
    "race_train = new_race[:val.astype(int)]\n",
    "\n",
    "#last 20% of the data\n",
    "race_test = new_race[val.astype(int):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of African-American that reoffend:  139\n",
      "# of African-American that did not reoffend:  198\n",
      "# of African-American that did not reoffend but were predicted to reoffend:  151\n",
      "# of African-American that did reoffend but were predicted not to reoffend:  256\n",
      "PPV:  47.93103448275862 %\n",
      "NPV:  43.61233480176212 %\n",
      "False Positive Parity:  43.26647564469914 %\n"
     ]
    }
   ],
   "source": [
    "#Find # of African-Americans that are predicted to reoffend vs. who actually reoffend\n",
    "actual = 0 #True Positive\n",
    "were_predicted = 0 #False Positive\n",
    "not_predicted = 0 #True Negative\n",
    "wrong_predict = 0 #False Negative\n",
    "\n",
    "for i in range(len(Data.x_test)):\n",
    "  if race_test[i] == 0: #If African-American\n",
    "    if a[i] >= 0.5: #If predicted to reoffend\n",
    "      if Data.y_test[i] == 1: #did reoffend\n",
    "        actual +=1\n",
    "      elif Data.y_test[i] == 0: #did not reoffend \n",
    "        were_predicted += 1\n",
    "    elif a[i] < 0.5: \n",
    "      if Data.y_test[i] == 0: #did not reoffend \n",
    "        not_predicted += 1\n",
    "      if Data.y_test[i] == 1: #did reoffend\n",
    "        wrong_predict += 1\n",
    "\n",
    "print(\"# of African-American that reoffend: \", actual)\n",
    "print(\"# of African-American that did not reoffend: \", not_predicted)\n",
    "print(\"# of African-American that did not reoffend but were predicted to reoffend: \", were_predicted)\n",
    "print(\"# of African-American that did reoffend but were predicted not to reoffend: \", wrong_predict)\n",
    "print(\"PPV: \", actual/(actual + were_predicted)*100, \"%\")\n",
    "print(\"NPV: \", not_predicted/(not_predicted + wrong_predict)*100, \"%\")\n",
    "print(\"False Positive Parity: \", were_predicted/(were_predicted + not_predicted)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find # of Caucasian Americans that are predicted to reoffend vs. who actually reoffend\n",
    "actual = 0 #True Positive\n",
    "were_predicted = 0 #False Positive\n",
    "not_predicted = 0 #True Negative\n",
    "wrong_predict = 0 #False Negative\n",
    "\n",
    "for i in range(len(Data.x_test)):\n",
    "  if race_test[i] == 1: #If Caucasian\n",
    "    if a[i] >= 0.5: #If predicted to reoffend\n",
    "      if Data.y_test[i] == 1: #did reoffend\n",
    "        actual +=1\n",
    "      elif Data.y_test[i] == 0: #did not reoffend\n",
    "        were_predicted += 1\n",
    "    elif a[i] < 0.5: \n",
    "      if Data.y_test[i] == 0: #did not reoffend \n",
    "        not_predicted += 1\n",
    "      if Data.y_test[i] == 1: #did reoffend\n",
    "        wrong_predict += 1\n",
    "\n",
    "print(\"# of Caucasians that reoffend: \", actual)\n",
    "print(\"# of Caucasians that did not reoffend: \", not_predicted)\n",
    "print(\"# of Caucasians that did not reoffend but were predicted to reoffend: \", were_predicted)\n",
    "print(\"# of Caucasians that did reoffend but were predicted not to reoffend: \", wrong_predict)\n",
    "print(\"PPV: \", actual/(actual + were_predicted)*100, \"%\")\n",
    "print(\"NPV: \", not_predicted/(not_predicted + wrong_predict)*100, \"%\")\n",
    "print(\"False Positive Parity: \", were_predicted/(were_predicted + not_predicted)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following seven features were used:\n",
    "\n",
    "age\n",
    "juv_fel_count\n",
    "juv_misd_count\n",
    "priors_count\n",
    "sex\n",
    "c_charge_degree\n",
    "c_charge_desc\n",
    "For the first five listed above, nothing was done. For sex and c_charge_desc, these string values were converted to integer values, which was easy due to their binary manner. The last one was difficult as there were over 400 unique charges. In order to combat this issue, one hot vectors were used where each unique charge was given its own unique vector that could be used in training and testing.\n",
    "\n",
    "The adversarial model followed the given procedure:\n",
    "\n",
    "Train the adversaral learning model: Linear->Sigmoid\n",
    "Train the network: Linear->Sigmoid->Linear->Adversal->Sigmoid\n",
    "A threshold of 50% was used for this trial, in order to compare the results to part 1, where only two attributes were used. As seen above, the FPR parity is < 5%, although the process in which the parity was calculated is the same as done in part 1. This shows that this model as well as increasing the number of features can improve these results by providing consistency between races in prediction calculations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
